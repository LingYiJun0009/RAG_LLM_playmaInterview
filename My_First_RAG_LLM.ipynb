{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sum2Xrc0Vf2j",
        "0_9RZRa6Pcm9",
        "jf7pTBgTSDT-"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LingYiJun0009/RAG_LLM_playmaInterview/blob/main/My_First_RAG_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replace huggingface Access Tokens to your own huggingface token\n",
        "### Then, remember to request access on the \"mistralai/Mistral-7B-Instruct-v0.2\", as it is gated\n"
      ],
      "metadata": {
        "id": "zgxzfz2uRk58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface_token = \"place token here\""
      ],
      "metadata": {
        "id": "P4iAQHjhTMm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Then, Press Run All to run everything\n",
        "### might take a couple minutes to download things and load model"
      ],
      "metadata": {
        "id": "sum2Xrc0Vf2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch sentence-transformers chromadb langchain streamlit pyngrok datasets accelerate bitsandbytes"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T09:14:56.262527Z",
          "iopub.execute_input": "2025-06-24T09:14:56.262712Z",
          "iopub.status.idle": "2025-06-24T09:16:51.419339Z",
          "shell.execute_reply.started": "2025-06-24T09:14:56.262695Z",
          "shell.execute_reply": "2025-06-24T09:16:51.418457Z"
        },
        "id": "xCNetGunPcm4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(huggingface_token)\n"
      ],
      "metadata": {
        "id": "YodRSBmHS3b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T09:16:51.421111Z",
          "iopub.execute_input": "2025-06-24T09:16:51.421545Z",
          "iopub.status.idle": "2025-06-24T09:17:21.011649Z",
          "shell.execute_reply.started": "2025-06-24T09:16:51.42152Z",
          "shell.execute_reply": "2025-06-24T09:17:21.011102Z"
        },
        "id": "S0yPWE48Pcm8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PatientCareRAGAgent:\n",
        "    def __init__(self):\n",
        "        self.setup_embedding_model()\n",
        "        self.setup_llm()\n",
        "        self.setup_vector_database()\n",
        "        self.patient_data = {}\n",
        "\n",
        "    def setup_embedding_model(self):\n",
        "        \"\"\"Initialize the embedding model for RAG\"\"\"\n",
        "        print(\"Loading embedding model...\")\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        print(\"✅ Embedding model loaded successfully!\")\n",
        "\n",
        "    def setup_llm(self):\n",
        "        \"\"\"Initialize the open-source LLM with efficient settings for Colab\"\"\"\n",
        "        print(\"Loading LLM model...\")\n",
        "\n",
        "        # Use quantization for better memory efficiency in Colab\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "\n",
        "        model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"cuda:0\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        # self.model = self.model.cuda()\n",
        "        # Add padding token if not present\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        print(\"✅ LLM model loaded successfully!\")\n",
        "\n",
        "    def setup_vector_database(self):\n",
        "        \"\"\"Initialize ChromaDB for vector storage\"\"\"\n",
        "        print(\"Setting up vector database...\")\n",
        "        self.chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
        "\n",
        "        collection_name = \"patient_care_data\"\n",
        "\n",
        "        try:\n",
        "            # Try to get existing collection first\n",
        "            self.collection = self.chroma_client.get_collection(name=collection_name)\n",
        "            print(f\"✅ Found existing collection: {collection_name}\")\n",
        "        except Exception:\n",
        "            # Collection doesn't exist, create it\n",
        "            print(f\"Collection '{collection_name}' not found. Creating new collection...\")\n",
        "            self.collection = self.chroma_client.create_collection(\n",
        "                name=collection_name,\n",
        "                metadata={\"hnsw:space\": \"cosine\"}\n",
        "            )\n",
        "            print(f\"✅ Created new collection: {collection_name}\")\n",
        "\n",
        "        print(\"✅ Vector database setup complete!\")\n",
        "    def get_ids(self):\n",
        "        # print out existing patient\n",
        "        all_ids = self.collection.get(include=[])['ids']\n",
        "\n",
        "        return(all_ids)\n",
        "\n",
        "    def add_patient_data(self, patient_id, data_entries):\n",
        "        \"\"\"Add patient data to the RAG system\"\"\"\n",
        "        print(f\"Adding data for patient {patient_id}...\")\n",
        "\n",
        "        documents = []\n",
        "        metadatas = []\n",
        "        ids = []\n",
        "\n",
        "        for i, entry in enumerate(data_entries):\n",
        "            # Create structured document\n",
        "            doc_text = f\"\"\"\n",
        "            Patient ID: {patient_id}\n",
        "            Category: {entry['category']}\n",
        "            Date: {entry['date']}\n",
        "            Information: {entry['content']}\n",
        "            Caregiver Notes: {entry.get('notes', 'N/A')}\n",
        "            \"\"\"\n",
        "\n",
        "            documents.append(doc_text)\n",
        "            metadatas.append({\n",
        "                'patient_id': patient_id,\n",
        "                'category': entry['category'],\n",
        "                'date': entry['date'],\n",
        "                'source': entry.get('source', 'manual_entry')\n",
        "            })\n",
        "            ids.append(f\"{patient_id}_{entry['category']}_{i}\")\n",
        "\n",
        "        # Generate embeddings and store\n",
        "        embeddings = self.embedding_model.encode(documents).tolist()\n",
        "\n",
        "        self.collection.add(\n",
        "            embeddings=embeddings,\n",
        "            documents=documents,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "\n",
        "        print(f\"✅ Added {len(documents)} entries for patient {patient_id}\")\n",
        "\n",
        "    def query_patient_info(self, patient_id, query, max_results=2):\n",
        "        \"\"\"Query patient information using RAG\"\"\"\n",
        "        print(f\"Querying information for patient {patient_id}: {query}\")\n",
        "\n",
        "        # Generate query embedding\n",
        "        query_embedding = self.embedding_model.encode([query]).tolist()\n",
        "\n",
        "        # Search vector database\n",
        "        results = self.collection.query(\n",
        "            query_embeddings=query_embedding,\n",
        "            n_results=max_results,\n",
        "            where={\"patient_id\": patient_id}\n",
        "        )\n",
        "\n",
        "        # Extract relevant context\n",
        "        context_docs = results['documents'][0] if results['documents'] else []\n",
        "\n",
        "        return self.generate_response(query, context_docs, patient_id)\n",
        "\n",
        "    def generate_response(self, query, context_docs, patient_id):\n",
        "        \"\"\"Generate response using LLM with retrieved context\"\"\"\n",
        "        device = \"cuda:0\"\n",
        "        # Prepare context\n",
        "        context = \"\\n\\n\".join(context_docs[:1])  # Use most relevant docs\n",
        "\n",
        "        prompt = f\"\"\"<s>[INST] You are a medical information assistant. Extract information from patient data to answer questions.\n",
        "\n",
        "        Rules:\n",
        "        - Answer ONLY based on the provided patient information\n",
        "        - If information is not available, respond with \"Information not available\"\n",
        "        - Be concise and factual\n",
        "        - Do not make assumptions or add information not explicitly stated\n",
        "\n",
        "        Patient Data:\n",
        "        {context}\n",
        "\n",
        "        Question: {query}\n",
        "\n",
        "        Provide a direct answer based only on the information above. [/INST]\"\"\"\n",
        "\n",
        "        print(f\"here is how the prompt look like: \\n {prompt}\")\n",
        "        # Tokenize and generate\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        inputs = inputs.to(device) # Move inputs to the same device as the model\n",
        "\n",
        "        print(f\"inputs device: {inputs.device}\")\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_length=inputs.shape[1] + 150,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        # print(f\"raw output: {outputs}\")\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extract just the response part\n",
        "        response = response.split(\"Response (be specific, helpful, and include relevant details from the patient history):\")[-1].strip()\n",
        "\n",
        "        return {\n",
        "            'response': response,\n",
        "            'context_used': len(context_docs),\n",
        "            'retrieved_info': context_docs\n",
        "        }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T09:17:21.165663Z",
          "iopub.execute_input": "2025-06-24T09:17:21.166525Z",
          "iopub.status.idle": "2025-06-24T09:17:21.510228Z",
          "shell.execute_reply.started": "2025-06-24T09:17:21.166508Z",
          "shell.execute_reply": "2025-06-24T09:17:21.509408Z"
        },
        "id": "BiW5Jy95Pcm9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# del agent  # Delete your large variables or dataframes\n",
        "# import gc\n",
        "# gc.collect()\n",
        "# import torch\n",
        "# torch.cuda.empty_cache()\n",
        "agent = PatientCareRAGAgent()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T09:17:21.5111Z",
          "iopub.execute_input": "2025-06-24T09:17:21.511363Z",
          "iopub.status.idle": "2025-06-24T09:19:17.675587Z",
          "shell.execute_reply.started": "2025-06-24T09:17:21.511339Z",
          "shell.execute_reply": "2025-06-24T09:19:17.674939Z"
        },
        "id": "MHBD5jbuPcm9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding patient data\n",
        "### Can add more data according to the format\n"
      ],
      "metadata": {
        "id": "0_9RZRa6Pcm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_patient_data = [\n",
        "    {\n",
        "        'category': 'medical_history',\n",
        "        'date': '2025-05-15',\n",
        "        'content': 'Patient has diabetes type 2, diagnosed in 2020. Takes metformin 500mg twice daily. Last HbA1c was 7.2%.',\n",
        "        'notes': 'Patient sometimes forgets evening medication'\n",
        "    },\n",
        "    {\n",
        "        'category': 'daily_habits',\n",
        "        'date': '2025-06-01',\n",
        "        'content': 'Prefers morning showers around 8 AM. Likes to have breakfast at 8:30 AM - usually oatmeal or toast.',\n",
        "        'notes': 'Gets agitated if routine is changed'\n",
        "    },\n",
        "    {\n",
        "        'category': 'preferences',\n",
        "        'date': '2025-06-10',\n",
        "        'content': 'Enjoys classical music, especially Mozart. Likes to read mystery novels in the afternoon.',\n",
        "        'notes': 'Music helps calm anxiety episodes'\n",
        "    },\n",
        "    {\n",
        "        'category': 'safety_concerns',\n",
        "        'date': '2025-06-15',\n",
        "        'content': 'Has history of falls. Uses walker for mobility. Needs assistance with stairs.',\n",
        "        'notes': 'Never leave unattended near stairs'\n",
        "    },\n",
        "    {\n",
        "        'category': 'social_info',\n",
        "        'date': '2025-06-12',\n",
        "        'content': 'Has two children who visit weekly. Daughter Sarah calls every Tuesday evening.',\n",
        "        'notes': 'Gets excited about family visits, may need help preparing'\n",
        "    }\n",
        "]\n",
        "\n",
        "# Add sample data\n",
        "patient_id = \"PATIENT_001\"\n",
        "agent.add_patient_data(patient_id, sample_patient_data)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T09:19:17.676287Z",
          "iopub.execute_input": "2025-06-24T09:19:17.676538Z",
          "iopub.status.idle": "2025-06-24T09:19:18.23586Z",
          "shell.execute_reply.started": "2025-06-24T09:19:17.676519Z",
          "shell.execute_reply": "2025-06-24T09:19:18.235275Z"
        },
        "id": "ls-BD1NFPcm-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_patient_data2 = [\n",
        "    {\n",
        "        'category': 'medical_history',\n",
        "        'date': '2025-05-20',\n",
        "        'content': 'Patient has hypertension, diagnosed in 2018. Takes amlodipine 10mg daily. Blood pressure averages 130/80 mmHg.',\n",
        "        'notes': 'Occasionally misses doses when feeling well'\n",
        "    },\n",
        "    {\n",
        "        'category': 'daily_habits',\n",
        "        'date': '2025-06-03',\n",
        "        'content': 'Enjoys taking a walk in the garden every evening at 6 PM. Has lunch at noon; prefers light meals like salads or soups.',\n",
        "        'notes': 'Walk is important for mood, avoid interruptions'\n",
        "    },\n",
        "    {\n",
        "        'category': 'preferences',\n",
        "        'date': '2025-06-11',\n",
        "        'content': 'Likes watching nature documentaries after dinner. Prefers herbal tea over coffee.',\n",
        "        'notes': 'Tea helps with relaxation before bedtime'\n",
        "    },\n",
        "    {\n",
        "        'category': 'safety_concerns',\n",
        "        'date': '2025-06-18',\n",
        "        'content': 'Has mild visual impairment. Needs good lighting in all rooms. Avoids using sharp kitchen tools.',\n",
        "        'notes': 'Ensure nightlights are on in hallways'\n",
        "    },\n",
        "    {\n",
        "        'category': 'social_info',\n",
        "        'date': '2025-06-14',\n",
        "        'content': 'Lives with spouse. Grandchildren visit on weekends. Enjoys video calls with old friends every Friday.',\n",
        "        'notes': 'May need help setting up video calls'\n",
        "    }\n",
        "]\n",
        "\n",
        "# Add sample data\n",
        "patient_id2 = \"PATIENT_002\"\n",
        "agent.add_patient_data(patient_id2, sample_patient_data2)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T09:19:18.23757Z",
          "iopub.execute_input": "2025-06-24T09:19:18.23778Z",
          "iopub.status.idle": "2025-06-24T09:19:18.290776Z",
          "shell.execute_reply.started": "2025-06-24T09:19:18.237765Z",
          "shell.execute_reply": "2025-06-24T09:19:18.290058Z"
        },
        "id": "ZBGSVflKPcm_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run query\n",
        "### Enter patient id first, then ask information about that patient\n",
        "#### Sample questions:\n",
        "##### - what's the patient's favourite food\n",
        "##### - Who are the patient's family\n",
        "##### - What illness does this patient have?\n",
        "etc..."
      ],
      "metadata": {
        "id": "jf7pTBgTSDT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_patient2(question, patient_id=\"PATIENT_001\"):\n",
        "    \"\"\"Helper function for easy querying\"\"\"\n",
        "\n",
        "    result = agent.query_patient_info(patient_id, question)\n",
        "\n",
        "    # print(f\"\\n❓ Question: {question}\")\n",
        "    # result = text.split(marker, 1)[1]\n",
        "    print(f\"🤖 Response: {result['response'].split('[/INST]')[1]}\")\n",
        "    # print(f\"📊 Used {result['context_used']} relevant documents\")\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T09:19:18.296706Z",
          "iopub.execute_input": "2025-06-24T09:19:18.296948Z",
          "iopub.status.idle": "2025-06-24T09:19:18.312266Z",
          "shell.execute_reply.started": "2025-06-24T09:19:18.296927Z",
          "shell.execute_reply": "2025-06-24T09:19:18.311711Z"
        },
        "id": "4nxl-PEDPcm_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\n",
        "patient_ID = \"\"\n",
        "while query != \"exit\":\n",
        "    unique_patients = sorted(list(set('_'.join(item.split('_')[:2]) for item in agent.get_ids())))\n",
        "\n",
        "    if patient_ID == \"\":\n",
        "        print(f\"sample patient ID: \\n{unique_patients}\")\n",
        "        patient_ID = input(\"please enter patient ID you'd like to query:\")\n",
        "        unique_patients = sorted(list(set('_'.join(item.split('_')[:2]) for item in agent.get_ids())))\n",
        "        if patient_ID not in unique_patients:\n",
        "            patient_ID = \"\"\n",
        "            print(f\"patient ID not found, here are the available choices: {unique_patients}\")\n",
        "            continue\n",
        "    print(\"enter 'exit' if you'd like to end session, or query another patient\")\n",
        "    query = input(\"Question:\" )\n",
        "    if query == \"exit\":\n",
        "        print(\"Session stopped: \")\n",
        "        continue\n",
        "    query_patient2(question = query, patient_id = patient_ID)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T09:19:42.1593Z",
          "iopub.execute_input": "2025-06-24T09:19:42.159653Z"
        },
        "id": "OoWq0p4EPcm_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "X8yySQ9PPcm_"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}